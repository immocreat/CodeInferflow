[main]
http_port = 8080
worker_count = 32
is_study_mode = false

global_model_dir = ${data_root_dir}../models/
default_prompt_template = {query}{\n}{res_prefix}

[transformer_engine]
; >>>> code models
; models = codegeex2-6b
; models = code_llama2_instruct_7b
models = starcoder2-3b
; models = deepseek_coder_7b_instruct_v1.5
; models = codeqwen1.5_7b

; >>>> chat models
;models = test
;models = gemma_2b_it
;models = llama2_7b_hf
;models = llama2_7b_chat_hf
;models = llama2_7b_chat.gguf
;models = llama2_13b_chat_hf
;models = open_llama_3b
;models = bloomz_3b
;models = baichuan2_7b_chat
;models = baichuan2_13b_chat
;models = mistral_7b_instruct
;models = mixtral_8x7b_instruct
;models = falcon_7b_instruct
;models = falcon_40b_instruct
;models = phi_2
;models = aquila_chat2_34b
; models = chatglm2_6b
;models = minicpm_2b_dpo_bf16
;models = xverse_13b_chat
;models = yi_6b_200k
;models = yi_34b_chat
;models = qwen1.5_0.5b_chat
; models = codeqwen1.5_7b_chat
;models = internlm_chat_20b
;models = fusellm_7b
;models = deepseek_moe_16b_base
;models = orion_14b_chat
; models = stories_15m.llama2_c
;models = stories_110m.llama2_c
;models = opt_350m
;models = opt_13b
;models = opt_iml_max_30b
;models = facebook_m2m100_418m
;models = bert_base_multilingual_cased

;devices = 0;1
devices = 0
decoder_cpu_layer_count = 0
cpu_threads = 8

max_concurrent_queries = 6

return_output_tensors = true

;debug options
is_study_mode = false
show_tensors = false

; ******************************************************************************************
; >>>> configs of code models

[model.codegeex2-6b]
model_dir = ${global_model_dir}/code/${model_name}/
model_specification_file = ${data_root_dir}../models/code/${model_name}/model_spec.json

;weight data types: {F32, F16, Q8, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F32, F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.fsd", "top_p":0.9}
prompt_template = {#64790}{#64792} #language:{code_language}{\n}{code_prefix}

[model.codeqwen1.5_7b]
model_dir = ${global_model_dir}/code/${model_name}/
model_specification_file = ${data_root_dir}../models/code/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = F16
device_kv_cache_data_type = F16
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.95}
prompt_template = {<fim_prefix>}{code_prefix}{<fim_suffix>}{code_suffix}{<fim_middle>}

[model.code_llama2_instruct_7b]
model_dir = ${global_model_dir}/code/${model_name}/
model_specification_file = ${data_root_dir}../models/code/${model_name}/model_spec.json

device_weight_data_type = F16
device_kv_cache_data_type = F16
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 512

prompt_template = {bos}#language: {code_language}{\n}{code_prefix}

[model.starcoder2-3b]
model_dir = ${global_model_dir}/code/${model_name}/
model_specification_file = ${data_root_dir}../models/code/${model_name}/model_spec.json

device_weight_data_type = F16
device_kv_cache_data_type = F16
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 512

; prompt_template = {code_prefix}
prompt_template = {<file_sep>}{<fim_prefix>}{code_file_path}{\n}{code_prefix}{<fim_suffix>}{code_suffix}{<fim_middle>}

[model.deepseek_coder_7b_instruct_v1.5]
model_dir = ${global_model_dir}/code/${model_name}/
model_specification_file = ${data_root_dir}../models/code/${model_name}/model_spec.json

device_weight_data_type = FP16
device_kv_cache_data_type = FP16
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 512
prompt_template = {<｜begin▁of▁sentence｜>}# language: {code_language}{\n}{code_prefix}



; ******************************************************************************************
; >>>> configs of chat models

[model.test]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.1900.json

device_weight_data_type = F16
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = {bos}{query}

[model.gemma_2b_it]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

max_context_len = 4096

;prompt_template = {bos}{query}
prompt_template = {bos}{query}{\n}

[model.llama2_7b_hf]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 512

;prompt_template = {_query} {res_prefix}
prompt_template = {bos}{query}
;prompt_template = llama2_chat.std
;prompt_template = llama_cpp_t0

[model.llama2_7b_chat_hf]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

;prompt_template = {_query} {res_prefix}
prompt_template = <s>[INST]{query}[/INST]
;prompt_template = llama2_chat.std
;prompt_template = llama_cpp_t0

[model.llama2_7b_chat.gguf]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = <s>[INST]{query}[/INST]

[model.llama2_13b_chat_hf]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = <s>[INST]{\n}{query}[/INST]{\n}
;prompt_template = llama2_chat.std

[model.open_llama_3b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = F16
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = {query} {res_prefix}

[model.stories_15m.llama2_c]
model_dir = ${global_model_dir}/chat/llama2.c/
model_specification_file = ${data_root_dir}../models/chat/llama2.c/model_spec_15m.json

device_weight_data_type =
host_kv_cache_percent = 0

prompt_template = {bos}#lang: {code_language}{\n}#filepath: {code_file_path}{\n}{code_prefix}

[model.stories_110m.llama2_c]
model_dir = ${global_model_dir}/chat/llama2.c/
model_specification_file = ${data_root_dir}../models/chat/llama2.c/model_spec_110m.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0

decoding_strategy = sample.std
prompt_template = {bos}{query}

[model.bloomz_3b]
model_dir = ${global_model_dir}/chat/${model_name}/
;model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.pickle.json
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.safetensors.json

device_weight_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

;devices = 0

decoding_strategy = sample.fsd
decoder_input_template = {query}{res_prefix}
;prompt_template = {query} {res_prefix}

[model.baichuan2_7b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

;max_context_len = 1024

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template ={\n}{\n}{<reserved_106>}{\n}{query}{<reserved_107>}{\n}
;prompt_template = {user_token}{query}{assistant_token}{res_prefix}

[model.baichuan2_13b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q5
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

;{BY_LAYER, BY_TENSOR}
;multi_gpu_strategy = BY_TENSOR

prompt_template = {\n}{\n}{<reserved_106>}{\n}{query}{<reserved_107>}{\n}
;prompt_template = {user_token}{query}{assistant_token}{res_prefix}

[model.mistral_7b_instruct]
model_dir = ${global_model_dir}/chat/mistral_7b_instruct_v0.2/
model_specification_file = ${data_root_dir}../models/chat/mistral_7b_instruct_v0.2/model_spec.safetensors.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
;device_weight_data_type = Q3H
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

devices = 0

decoding_strategy = sample.top_p
prompt_template = <s> [INST] {query} [/INST]

[model.mixtral_8x7b_instruct]
model_dir = ${global_model_dir}/chat/mixtral_8x7b_instruct_v0.1/
model_specification_file = ${data_root_dir}../models/chat/mixtral_8x7b_instruct_v0.1/model_spec.safetensors.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q3H
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

decoding_strategy = sample.top_p
prompt_template = {<s>}{query}

[model.falcon_7b_instruct]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = sample.fsd
prompt_template = {query}

[model.falcon_40b_instruct]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q3H
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

;devices = 0&1;2&3

decoding_strategy = sample.top_p
prompt_template = {query}

[model.phi_2]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.safetensors.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

;devices = 0&1;2&3

decoding_strategy = sample.top_p
prompt_template = Instruct: {query}{\n}Output:

[model.aquila_chat2_34b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q3H
;device_weight_data_type = Q8_B32T2
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {query}

[model.chatglm2_6b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F32, F16, Q8, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F32, F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.fsd", "top_p":0.9}
; prompt_template = {#64790}{#64792} [Round 1]{\n}{\n}问：{query}{\n}{\n}答：
; prompt_template = {#64790}{#64792} {query}
prompt_template = {#64790}{#64792} #language:{code_language}{\n}{code_prefix}
; prompt_template = {#64790}{#64792} {code_prefix}

[model.minicpm_2b_dpo_bf16]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9}
prompt_template = {<s>} <用户>{query}<AI>

[model.xverse_13b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {query}{\n}Assistant:
;prompt_template = {User}:{\n}{query}{\n}Assistant:{\n}

[model.yi_6b_200k]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = F16
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.fsd", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = {bos}{query}

[model.yi_34b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q3H
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

max_context_len = 4096

decoding_strategy = {"name":"sample.top_p", "top_p":0.9, "eos_bypassing_max":0}
prompt_template = <|im_start|>system{\n}{system_prompt}<|im_end|>{\n}<|im_start|>user{\n}{query}<|im_end|>{\n}<|im_start|>assistant{\n}

[model.qwen1.5_0.5b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9}
prompt_template = {<|im_start|>}user{\n}{query}{<|im_end|>}{\n}{<|im_start|>}assistant{\n}

[model.codeqwen1.5_7b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
;kv-cache data types: {F16, Q8}
device_weight_data_type = F16
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p", "top_p":0.9}
; prompt_template = {<|im_start|>}user{\n}{query}{<|im_end|>}{\n}{<|im_start|>}assistant{\n}
; prompt_template = {<|im_start|>}user{\n}{code_prefix}{<|im_end|>}{\n}{<|im_start|>}assistant{\n}
prompt_template = {<fim_prefix>}{code_prefix}{<fim_suffix>}{code_suffix}{<fim_middle>}

[model.internlm_chat_20b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;{F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q5
;{F16, Q8}
device_kv_cache_data_type = Q8
;delta_tensor_ratio = 0.001
host_kv_cache_percent = 100
be_host_embeddings = true

decoding_strategy = sample.top_p
prompt_template = <s><|User|>:{query}\n<|Bot|>:

[model.fusellm_7b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = {bos}{query}
;prompt_template = <s>[INST]{query}[/INST]

[model.deepseek_moe_16b_base]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

device_weight_data_type = Q5
device_kv_cache_data_type = Q8
tensor_quant_threshold = 0
host_kv_cache_percent = 0
be_host_embeddings = true

prompt_template = {bos}{query}

[model.opt_350m]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.greedy"}
prompt_template = {bos}{query}

[model.opt_13b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.greedy"}
prompt_template = {bos}{query}

[model.opt_iml_max_30b]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q3H
device_kv_cache_data_type = Q8
host_kv_cache_percent = 50
be_host_embeddings = true

decoding_strategy = {"name":"sample.greedy"}
prompt_template = {bos}{query}{\n}A:{\n}
;prompt_template = {bos}{query}{\n}A:

[model.orion_14b_chat]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.top_p"}
prompt_template = {query}{\n}{res_prefix}

[model.facebook_m2m100_418m]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

;weight data types: {F16, Q8, Q6, Q5, Q4, Q3H, Q3, Q2}
device_weight_data_type = Q8_B32T2
host_kv_cache_percent = 0
be_host_embeddings = true

decoding_strategy = {"name":"sample.std", "top_p":0.9, "eos_bypassing_max":0}
encoder_input_template = {__zh__}{ }{query}{</s>}
decoder_input_template = {#2}{__en__}{res_prefix}
;encoder_input_template = {__en__}{ }{query}{</s>}
;decoder_input_template = {#2}{__fr__}{res_prefix}
;prompt_template = {query}{res_prefix}

[model.bert_base_multilingual_cased]
model_dir = ${global_model_dir}/chat/${model_name}/
model_specification_file = ${data_root_dir}../models/chat/${model_name}/model_spec.json

encoder_input_template = {[CLS]}{query}{[SEP]}

[prompt_templates]
prompt_template_count = 11
prompt_template1 = simple
prompt_template2 = llama2_chat.std
prompt_template3 = llama2_chat.std.1
prompt_template4 = llama2_chat.std.2
prompt_template5 = llama2_chat.alt
prompt_template6 = llama2_chat.alt.2
prompt_template7 = vicuna-1.0
prompt_template8 = vicuna-1.1
prompt_template9 = llama_cpp_t0
prompt_template10 = llama_cpp_t1
prompt_template11 = baichuan2_chat

[prompt_template.simple]
line_count = 2
line1 = {query}
line2 = {res_prefix}

[prompt_template.llama2_chat.std]
line_count = 2
line1 = <s>[INST]
line2 = {query}[/INST]

[prompt_template.llama2_chat.std.1]
line_count = 4
line1 = <s>[INST]<<SYS>>
line2 = {system_prompt}
line3 = <</SYS>>
line4 = {query}[/INST]

[prompt_template.llama2_chat.std.2]
line_count = 5
line1 = <s>[INST]<<SYS>>
line2 = You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
line3 = If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
line4 = <</SYS>>
line5 = {query}[/INST]

[prompt_template.llama2_chat.alt]
line_count = 2
line1 = USER: {query}
line2 = ASSISTANT: {res_prefix}

[prompt_template.llama2_chat.alt.2]
line_count = 3
line1 = SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
line2 = USER: {query}
line3 = ASSISTANT: {res_prefix}

[prompt_template.vicuna-1.0]
line_count = 4
line1 = ### Human:
line2 = {query}
line3 = ### Assistant:
line4 = {res_prefix}

[prompt_template.vicuna-1.1]
line_count = 2
line1 = USER: {query}
line2 = ASSISTANT: {res_prefix}

[prompt_template.llama_cpp_t0]
line_count = 1
line1 = \n\n### Instruction:\n\n{query}\n\n### Response:\n\n{res_prefix}

[prompt_template.llama_cpp_t1]
line_count = 1 
line1 = \n\n### Instruction:\n\n{_query}\n\n### Response:\n\n{res_prefix}

[prompt_template.baichuan2_chat]
line_count = 1 
line1 = \n\n{<reserved_106>}{query}{<reserved_107>}\n

;//////////////////////////////////////////////////////////////////////////////
;// Application Environment
;//////////////////////////////////////////////////////////////////////////////

[app_env.base]
data_root_dir = ${config_dir}../data/inferflow/
require_enter_key_to_exit = true

[app_env.logging]
enable_logging = 1
log_dir = ${data_root_dir}logs
log_name = ${app_name}

[app_env.status_manager]
enable_monitoring = 0
status_file = ${data_root_dir}/${app_name}.status
listening_port = 9877
