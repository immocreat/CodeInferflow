[main]
inference_engine_config = ${config_dir}/llm_inference.tiny.ini

query_list = query_list

query_random_seed = 1
temperature = 0.7

long_context_test = false

[query_list]
query_count = 1
query1 = {"res_prefix":"Today is a nice"}
;query1 = {"res_prefix":"On a farm long ago, a Mama Duck sat on her nest.  “How long must I wait for my babies to hatch?” she said.  “I have to sit here all alone! And no one comes to visit me.”  But what could she do? A Mama duck must keep her eggs warm till they hatch."}

[transformer_engine]
;models = stories_15m.llama2_c
models = stories_110m.llama2_c

devices = 0
decoder_cpu_layer_count = 0
cpu_threads = 8

max_concurrent_queries = 6

return_output_tensors = false

;debug options
is_study_mode = false
show_tensors = false

[model.stories_15m.llama2_c]
model_dir = ${data_root_dir}../models/llama2.c/
model_specification_file = model_spec_15m.json

device_weight_data_type = F16
device_kv_cache_data_type = F16
host_kv_cache_percent = 0

prompt_template = {bos}{query}

[model.stories_110m.llama2_c]
model_dir = ${data_root_dir}../models/llama2.c/
model_specification_file = model_spec_110m.json

device_weight_data_type = Q8
device_kv_cache_data_type = Q8
host_kv_cache_percent = 0

decoding_strategy = sample.std
prompt_template = {bos}{query}

[prompt_templates]
prompt_template_count = 0

;//////////////////////////////////////////////////////////////////////////////
;// Application Environment
;//////////////////////////////////////////////////////////////////////////////

[app_env.base]
data_root_dir = ${config_dir}../data/inferflow/
require_enter_key_to_exit = true

[app_env.logging]
enable_logging = 1
log_dir = ${data_root_dir}logs
log_name = ${app_name}

[app_env.status_manager]
enable_monitoring = 0
status_file = ${data_root_dir}/${app_name}.status
listening_port = 9877
